---
title: "Continuation"
theme: white
plugins:
  - highlight
  - math
---
---

# Continuation

---

## SGD isn't perfect

<img src="{{ site.baseurl}}/assets/mnist.png" style="padding: 25px; height: 400px;">

---

<img src="{{ site.baseurl}}/assets/cifar.png" style="padding: 25px; height: 400px;">

---

## Continuation AKA graduated optimization

* Rough parallel to simulated annealing

<img src="{{ site.baseurl}}/assets/graduated.png" style="padding: 25px; height: 400px;">

---

## Analytic solution

1. Convolve with e.g. a Gaussian kernel
2. Find minimum
3. Reduce kernel size and repeat

Depends on $\sigma$-niceness/optimization complexity.

See e.g. *A Theoretical Analysis of Optimization by Gaussian Continuation* by
Mobahi and Fisher.

---

## Stochastic regime

* Use sampling to estimate smoothing, $\hat \nabla f(x) = \mathbb{E}_\epsilon(x + \epsilon)$
* Reduce smoothing gradually?

See e.g. *On Graduated Optimization for Stochastic Non-Convex Problems* by
Hazan, Levy, Shalev-Swartz

---

## Potential issues

* Number of samples needed for Monte Carlo estimate grows with dimension
